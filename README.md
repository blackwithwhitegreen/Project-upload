# VectorAsk-

VectorAsk is an interactive Streamlit web application that allows users to query uploaded PDF documents using a retrieval-augmented question-answering system powered by a local Hugging Face model (google/flan-t5-base). The app provides contextual answers along with visual source page evidence rendered directly from the uploaded PDF.

![Image](https://github.com/user-attachments/assets/6ff1914e-e57c-46bc-9a03-9232a5df0663)



# Features

Upload any PDF document and extract answers based on its content.

Uses FAISS for semantic document indexing.

Answers generated by google/flan-t5-base, served via transformers pipeline.

Splits documents into manageable chunks using RecursiveCharacterTextSplitter.

Renders the actual source page images to justify the answers.

Also supports general knowledge questions via the same model.


# Demo
Run the app with:



    streamlit run app.py
Replace app.py with your actual Python filename.


# How It Works

PDF Upload: User uploads a PDF which is parsed and split into chunks.

Embedding: Chunks are embedded using sentence-transformers/all-MiniLM-L6-v2.

Vector Store: FAISS is used to create a searchable vector index.

RAG Pipeline: The langchain RetrievalQA chain retrieves relevant chunks and uses a Hugging Face pipeline to generate answers.

Source Pages: The app renders the corresponding PDF pages as images using PyMuPDF.


# Dependencies

Install dependencies from the requirements.txt:


    pip install -r requirements.txt

`requirements.txt`

    streamlit>=1.32.0
    pypdf>=4.1.0
    langchain>=0.1.0
    langchain-community>=0.0.20
    langchain-core>=0.1.0
    langchain-text-splitters>=0.0.1
    faiss-cpu>=1.8.0
    sentence-transformers>=2.3.1
    transformers>=4.38.0
    torch>=2.2.0
    accelerate>=0.27.0
    pymupdf>=1.23.0
    pillow>=10.0.0


# Project Structure

    VectorAsk/
    │
    ├── app.py                # Main Streamlit app
    ├── requirements.txt      # Dependencies
    └── README.md             # You're reading it!


# Configuration
Change model or embedding model in app.py:

    MODEL_NAME = "google/flan-t5-base"
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# Modify chunk size or overlap:

    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 50

# Model Info

LLM: FLAN-T5 Base

Embeddings: all-MiniLM-L6-v2



# License

This project is licensed under the MIT License. See the LICENSE file for details.


